{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total token amount: 3077\n",
      "3077\n",
      "<URL>  ->  526859\n",
      "<3  ->  57215\n",
      ":D  ->  24443\n",
      ":P  ->  13333\n",
      ":/  ->  9655\n",
      ":')  ->  3982\n",
      "=)  ->  2595\n",
      ";D  ->  1925\n",
      "<---  ->  1768\n",
      "--->  ->  1674\n",
      ":|  ->  1504\n",
      ";P  ->  1089\n",
      "-->  ->  1022\n",
      "->  ->  992\n",
      "::  ->  739\n",
      "<--  ->  666\n",
      ":-D  ->  490\n",
      "01  ->  449\n",
      "00  ->  447\n",
      ":\\  ->  410\n",
      ":-P  ->  406\n",
      "02  ->  391\n",
      ":]  ->  386\n",
      "=D  ->  375\n",
      ":'D  ->  358\n",
      ":-/  ->  350\n",
      "0.5  ->  338\n",
      "<-  ->  336\n",
      "000  ->  297\n",
      ":@  ->  259\n",
      "07  ->  238\n",
      "09  ->  231\n",
      "05  ->  225\n",
      "03  ->  222\n",
      "=(  ->  220\n",
      "04  ->  220\n",
      "08  ->  218\n",
      "06  ->  206\n",
      ";/  ->  176\n",
      "0.0  ->  173\n",
      "=P  ->  158\n",
      "0BK  ->  156\n",
      "=/  ->  153\n",
      "001  ->  152\n",
      "=]  ->  134\n",
      "0MAH  ->  114\n",
      ":}  ->  111\n",
      "['<URL>', '<3', ':D', ':P', ':/', \":')\", '=)', ';D', '<---', '--->', ':|', ';P', '-->', '->', '::', '<--', ':-D', '01', '00', ':\\\\', ':-P', '02', ':]', '=D', \":'D\", ':-/', '0.5', '<-', '000', ':@', '07', '09', '05', '03', '=(', '04', '08', '06', ';/', '0.0', '=P', '0BK', '=/', '001', '=]', '0MAH', ':}']\n"
     ]
    }
   ],
   "source": [
    "# analyse the most frequent emojis in the training dataset\n",
    "\n",
    "import string\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# List of file names\n",
    "files = [\"../twitter-datasets/train_neg_full.txt\", \"../twitter-datasets/train_pos_full.txt\"]  # Update with your filenames\n",
    "\n",
    "# Create an empty dictionary to hold counts\n",
    "token_counts = defaultdict(int)\n",
    "\n",
    "# Define the special characters\n",
    "special_chars = \":;<-\\\\^-=0*+\"\n",
    "\n",
    "# Iterate over the files\n",
    "for filename in files:\n",
    "    \n",
    "    with open(filename, \"r\") as file:\n",
    "        # Read the file\n",
    "        data = file.read()\n",
    "        \n",
    "        # Tokenize the text file content by splitting at space\n",
    "        tokens = data.split()\n",
    "        \n",
    "        # Iterate over tokens\n",
    "        for token in tokens:\n",
    "            # Check if the length of the token is between 2 and 5\n",
    "            # and if it contains any special character\n",
    "            if 2 <= len(token) <= 5 and any(token[0] == char for char in special_chars):\n",
    "                # Increment the count of the token\n",
    "                token_counts[token] += 1\n",
    "\n",
    "# Print the dictionary\n",
    "print(f'total token amount: {len(token_counts)}')\n",
    "\n",
    "# Print the results and save the most frequent tokens into an array\n",
    "emoji_list = []\n",
    "\n",
    "print(len(token_counts))\n",
    "for token, count in sorted(token_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 100:\n",
    "        print(f\"{token.upper()}  ->  {count}\")\n",
    "        emoji_list.append(token.upper())\n",
    "print(emoji_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New file without emoji saved as ../twitter-datasets/train_neg_full_without_emoji.txt\n",
      "New file without emoji saved as ../twitter-datasets/train_pos_full_without_emoji.txt\n"
     ]
    }
   ],
   "source": [
    "# manuelly delete the tokens in the emoji_list that do not make sense as emojis, e.g. \"<URL>\", \"00\", \"01\"\n",
    "# map the emojis to very simple English adjectives\n",
    "# create a key: value dictionary\n",
    "\n",
    "emoji_dict = {\n",
    "    '<3': 'Lovely',\n",
    "    ':D': 'Happy',\n",
    "    ':P': 'Playful',\n",
    "    ':/': 'Unsure',\n",
    "    \":')\": 'Heartwarming',\n",
    "    '=)': 'Content',\n",
    "    ';D': 'Cheeky',\n",
    "    ':|': 'Neutral',\n",
    "    ';P': 'Teasing',\n",
    "    ':-D': 'Excited',\n",
    "    ':\\\\': 'Annoyed',\n",
    "    ':-P': 'Joking',\n",
    "    ':]': 'Happy',\n",
    "    '=D': 'Excited',\n",
    "    \":'D\": 'Delighted',\n",
    "    ':-/': 'Puzzled',\n",
    "    '=(': 'Sad',\n",
    "    ';/': 'Disappointed',\n",
    "    '0.0': 'Surprised',\n",
    "    '=P': 'Amused',\n",
    "    '=/': 'Uneasy',\n",
    "    '=]': 'Optimistic',\n",
    "    ':}': 'Smug'\n",
    "}\n",
    "\n",
    "# method for replacing the emojis with adjectives\n",
    "def replace_emoji(dictionary, text):\n",
    "    for key, value in dictionary.items():\n",
    "        text = text.replace(key.lower(), value.lower())\n",
    "    return text\n",
    "\n",
    "def get_file_without_emoji(filename):\n",
    "    output_filename = filename.replace(\".txt\", \"_without_emoji.txt\")        \n",
    "    \n",
    "    with open(filename, 'r') as input_file:\n",
    "        lines = input_file.readlines()\n",
    "    \n",
    "    modified_lines = []\n",
    "    for line in lines:\n",
    "        line_without_emoji = replace_emoji(emoji_dict, line)\n",
    "        modified_lines.append(line_without_emoji)\n",
    "    \n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        output_file.writelines(modified_lines)\n",
    "    \n",
    "    print(f\"New file without emoji saved as {output_filename}\")\n",
    "\n",
    "files = [\"../twitter-datasets/train_neg_full.txt\", \"../twitter-datasets/train_pos_full.txt\"]  # Update with your filenames\n",
    "\n",
    "for filename in files:\n",
    "    get_file_without_emoji(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average hashtags per tweet: 0.1458836\n",
      "Median hashtags per tweet: 0.0\n",
      "50.0% of hashtags occur 1.0 times or less\n",
      "75.0% of hashtags occur 1.0 times or less\n",
      "76.0% of hashtags occur 1.0 times or less\n",
      "77.0% of hashtags occur 1.0 times or less\n",
      "78.0% of hashtags occur 2.0 times or less\n",
      "79.0% of hashtags occur 2.0 times or less\n",
      "80.0% of hashtags occur 2.0 times or less\n",
      "85.0% of hashtags occur 2.0 times or less\n",
      "90.0% of hashtags occur 3.0 times or less\n",
      "95.0% of hashtags occur 6.0 times or less\n",
      "97.0% of hashtags occur 10.0 times or less\n",
      "98.0% of hashtags occur 14.0 times or less\n",
      "99.0% of hashtags occur 27.0 times or less\n",
      "99.5% of hashtags occur 50.0 times or less\n",
      "99.9% of hashtags occur 180.26800000004005 times or less\n",
      "Total amount of unique hashtags: 129367 total amount of hashtags: 364709\n",
      "#  ->  10739\n",
      "#oomf  ->  4242\n",
      "#ff  ->  3960\n",
      "#yougetmajorpointsif  ->  3378\n",
      "#thoughtsduringschool  ->  1763\n",
      "#smartnokialumia  ->  1620\n",
      "#\n",
      "  ->  1492\n",
      "#sadtweet\n",
      "  ->  1441\n",
      "#waystomakemehappy  ->  1420\n",
      "#teamfollowback  ->  1332\n",
      "#np  ->  1055\n",
      "#imagine  ->  970\n",
      "#believe  ->  915\n",
      "#teamfollowback\n",
      "  ->  876\n",
      "#jujur\n",
      "  ->  872\n",
      "#cantsayno  ->  857\n",
      "#shoutout  ->  797\n",
      "#rt  ->  794\n",
      "#fml\n",
      "  ->  667\n",
      "#follow  ->  648\n",
      "#nf  ->  640\n",
      "#jobs\n",
      "  ->  638\n",
      "#jobs  ->  599\n",
      "#fb\n",
      "  ->  577\n",
      "#harrypotterchatuplines  ->  565\n",
      "#nowplaying  ->  564\n",
      "#yougetmajorpoints  ->  559\n",
      "#fcblive\n",
      "  ->  540\n",
      "#10thingsiwanttohappen  ->  490\n",
      "#ifindthatattractive\n",
      "  ->  484\n",
      "#boyfriend  ->  483\n",
      "#mentionto  ->  479\n",
      "#ripmrsbieber  ->  461\n",
      "#sad\n",
      "  ->  458\n",
      "#theluckyone  ->  447\n",
      "#winning\n",
      "  ->  431\n",
      "#loveyou\n",
      "  ->  417\n",
      "#oomf\n",
      "  ->  415\n",
      "#ff\n",
      "  ->  415\n",
      "#adoptakuola  ->  411\n",
      "#quran\n",
      "  ->  410\n",
      "#youcangetmajorpointsif  ->  405\n",
      "#followback  ->  384\n",
      "#ifindthatattractive  ->  378\n",
      "#openfollow  ->  371\n",
      "#retweet  ->  370\n",
      "#love  ->  369\n",
      "#gigmemories  ->  369\n",
      "#yolo  ->  358\n",
      "#excited\n",
      "  ->  358\n",
      "#sadtweet  ->  348\n",
      "#420  ->  343\n",
      "#itssadthat  ->  340\n",
      "#yolo\n",
      "  ->  340\n",
      "#waystobeginsex  ->  338\n",
      "#mybiggestfearis  ->  325\n",
      "#scorpio  ->  324\n",
      "#happytweet\n",
      "  ->  324\n",
      "#nw  ->  323\n",
      "#throwbackthursday  ->  323\n",
      "#fml  ->  312\n",
      "#followme  ->  296\n",
      "#goodnight\n",
      "  ->  287\n",
      "#thevoice  ->  285\n",
      "#glee  ->  283\n",
      "#justsaying\n",
      "  ->  281\n",
      "#tbt  ->  281\n",
      "#pbbteens4  ->  281\n",
      "#hg\n",
      "  ->  279\n",
      "#muchlove  ->  278\n",
      "#ifollowback  ->  277\n",
      "#christianpickuplines\n",
      "  ->  271\n",
      "#tfb  ->  269\n",
      "#1dfamily  ->  268\n",
      "#youknowicarewhen  ->  266\n",
      "#job  ->  263\n",
      "#cleancloud  ->  259\n",
      "#liamfollow  ->  255\n",
      "#winning  ->  252\n",
      "#tayong  ->  252\n",
      "#loveyou  ->  250\n",
      "#sadtimes\n",
      "  ->  249\n",
      "#fb  ->  239\n",
      "#happy\n",
      "  ->  239\n",
      "#jfb  ->  237\n",
      "#nowfollowing  ->  237\n",
      "#pisces  ->  236\n",
      "#excited  ->  233\n",
      "#ificanthaveyou  ->  233\n",
      "#thevoiceuk\n",
      "  ->  231\n",
      "#tvd  ->  231\n",
      "#boyfriendvideo  ->  229\n",
      "#youknowwhatannoysme  ->  229\n",
      "#muchlove\n",
      "  ->  229\n",
      "#bgt\n",
      "  ->  225\n",
      "#cantwait\n",
      "  ->  225\n",
      "#blessed\n",
      "  ->  224\n",
      "#foreveralone\n",
      "  ->  221\n",
      "#mlb\n",
      "  ->  221\n",
      "#sad  ->  218\n",
      "#thinklikeaman  ->  217\n",
      "#thevoice\n",
      "  ->  216\n",
      "#orangotag\n",
      "  ->  216\n",
      "#towie  ->  215\n",
      "#ouch\n",
      "  ->  215\n",
      "#neversaynever  ->  215\n",
      "#immadbecause  ->  213\n",
      "#news\n",
      "  ->  209\n",
      "#fail\n",
      "  ->  208\n",
      "#idol  ->  207\n",
      "#10  ->  205\n",
      "#fcblive  ->  204\n",
      "#love\n",
      "  ->  203\n",
      "#aries  ->  202\n",
      "#askzacefron  ->  200\n",
      "#nba\n",
      "  ->  198\n",
      "#1000aday  ->  196\n",
      "#free  ->  195\n",
      "#chasingthesun  ->  192\n",
      "#420\n",
      "  ->  192\n",
      "#cantsayno\n",
      "  ->  190\n",
      "#missyou\n",
      "  ->  190\n",
      "#goodnight  ->  190\n",
      "#aintlovecrazy\n",
      "  ->  190\n",
      "#blessed  ->  189\n",
      "#pbbtweets  ->  189\n",
      "#1dnextalbumtitle  ->  188\n",
      "#please  ->  185\n",
      "#firstworldproblems\n",
      "  ->  182\n",
      "#please\n",
      "  ->  181\n",
      "#thewantedonthevoice  ->  179\n",
      "#twfanmily  ->  178\n",
      "#500aday  ->  176\n",
      "#globelumia800  ->  176\n",
      "#rip\n",
      "  ->  175\n",
      "#sorry\n",
      "  ->  174\n",
      "#justsaying  ->  174\n",
      "#sosad\n",
      "  ->  173\n",
      "#lol  ->  171\n",
      "#iloveyou\n",
      "  ->  169\n",
      "#wah\n",
      "  ->  167\n",
      "#rip  ->  166\n",
      "#happy  ->  166\n",
      "#lifeonmurs  ->  166\n",
      "#win  ->  166\n",
      "#music  ->  165\n",
      "#goodtimes\n",
      "  ->  165\n",
      "#idol\n",
      "  ->  164\n",
      "#sorrynotsorry\n",
      "  ->  164\n",
      "#lol\n",
      "  ->  162\n",
      "#pbbteens4\n",
      "  ->  160\n",
      "#ipl  ->  159\n",
      "#nhl\n",
      "  ->  156\n",
      "#loveit\n",
      "  ->  156\n",
      "#instagram  ->  155\n",
      "#iloveyou  ->  153\n",
      "#gutted\n",
      "  ->  153\n",
      "#5bestsmells  ->  153\n",
      "#yay\n",
      "  ->  153\n",
      "#help\n",
      "  ->  152\n",
      "#proud\n",
      "  ->  152\n",
      "#biggestregret  ->  148\n",
      "#happy420  ->  147\n",
      "#np\n",
      "  ->  146\n",
      "#desperatehousewives  ->  145\n",
      "#justsayin\n",
      "  ->  145\n",
      "#sagittarius  ->  145\n",
      "#tired\n",
      "  ->  144\n",
      "#itscrazyhow  ->  144\n",
      "#whaticanseerightnow  ->  144\n",
      "#believe\n",
      "  ->  142\n",
      "#rt\n",
      "  ->  142\n",
      "#lt  ->  142\n",
      "#instantfollowback\n",
      "  ->  142\n",
      "#dailytweet\n",
      "  ->  141\n",
      "#glee\n",
      "  ->  140\n",
      "#1dwebstersurfboards  ->  140\n",
      "#greatboobs  ->  140\n",
      "#wahhh\n",
      "  ->  139\n",
      "#jealous\n",
      "  ->  139\n",
      "#badtimes\n",
      "  ->  138\n",
      "#fail  ->  137\n",
      "#memories\n",
      "  ->  137\n",
      "#sadface\n",
      "  ->  136\n",
      "#mentionke  ->  136\n",
      "#followback\n",
      "  ->  135\n",
      "#10factsaboutme  ->  134\n",
      "#nsn  ->  134\n",
      "#cantwait  ->  134\n",
      "#gemini  ->  134\n",
      "#proud  ->  134\n",
      "#1dfamily\n",
      "  ->  133\n",
      "#nfb  ->  133\n",
      "#jealous  ->  132\n",
      "#ohwell\n",
      "  ->  132\n",
      "#realtalk\n",
      "  ->  130\n",
      "#1dfact  ->  130\n",
      "#oomfs  ->  130\n",
      "#thanks\n",
      "  ->  129\n",
      "#heartbroken\n",
      "  ->  128\n",
      "#icriedwhen  ->  128\n",
      "#job\n",
      "  ->  127\n",
      "#unattractivethingsaboutme  ->  127\n",
      "#twitter  ->  127\n",
      "#instantfollowback  ->  127\n",
      "#whitekenny  ->  127\n",
      "#sick\n",
      "  ->  126\n",
      "#thevoiceuk  ->  126\n",
      "#iadmit  ->  125\n",
      "#ugh\n",
      "  ->  125\n",
      "#giveaway  ->  125\n",
      "#loveher\n",
      "  ->  125\n",
      "#lonely\n",
      "  ->  123\n",
      "#uber2012  ->  123\n",
      "#bored\n",
      "  ->  123\n",
      "#depressed\n",
      "  ->  122\n",
      "#news  ->  122\n",
      "#boo\n",
      "  ->  122\n",
      "#fact  ->  122\n",
      "#castings  ->  121\n",
      "#finally\n",
      "  ->  121\n",
      "#leo  ->  120\n",
      "#bgt  ->  119\n",
      "#scared\n",
      "  ->  118\n",
      "#50liesiwastold  ->  118\n",
      "#followpenshoppeallstars  ->  118\n",
      "#towie\n",
      "  ->  117\n",
      "#tired  ->  117\n",
      "#onedirection  ->  117\n",
      "#followfriday  ->  117\n",
      "#finchel  ->  117\n",
      "#truth\n",
      "  ->  116\n",
      "#niallfact  ->  116\n",
      "#swag  ->  116\n",
      "#allrelationshipsneed  ->  116\n",
      "#foreveralone  ->  115\n",
      "#thewantedep4days  ->  115\n",
      "#f1  ->  115\n",
      "#wtf  ->  114\n",
      "#yay  ->  114\n",
      "#indonesianidol  ->  114\n",
      "#tvd\n",
      "  ->  113\n",
      "#imissyou\n",
      "  ->  113\n",
      "#bahrain  ->  113\n",
      "#bieberfact  ->  113\n",
      "#canucks  ->  112\n",
      "#lrt  ->  112\n",
      "#autofollowback  ->  111\n",
      "#goodmorning  ->  110\n",
      "#sorryforbashing  ->  110\n",
      "#taurus  ->  110\n",
      "#teamautofollow  ->  110\n",
      "#sometimesijustwant  ->  108\n",
      "#americanidol  ->  108\n",
      "#followme\n",
      "  ->  108\n",
      "#ucl\n",
      "  ->  108\n",
      "#autofollow  ->  107\n",
      "#thatawkwardmoment  ->  107\n",
      "#virgo  ->  107\n",
      "#depressing\n",
      "  ->  106\n",
      "#ohwell  ->  106\n",
      "#ouch  ->  106\n",
      "#notcool\n",
      "  ->  106\n",
      "#fact\n",
      "  ->  106\n",
      "#cute  ->  106\n",
      "#ss4inaday2  ->  105\n",
      "#crying\n",
      "  ->  104\n",
      "#1dfacts  ->  104\n",
      "#gutted  ->  103\n",
      "#disappointed\n",
      "  ->  103\n",
      "#thankyou\n",
      "  ->  103\n",
      "#usvote4jessicasanchez  ->  103\n",
      "#f4f  ->  102\n",
      "#scorpios  ->  102\n",
      "#amazing  ->  101\n",
      "#swag\n",
      "  ->  101\n",
      "#smh  ->  101\n",
      "#shutupandkissme  ->  101\n"
     ]
    }
   ],
   "source": [
    "# analysing hashtag frequency\n",
    "\n",
    "from collections import defaultdict\n",
    "from statistics import mean, median\n",
    "import numpy as np\n",
    "\n",
    "files = [\"../twitter-datasets/train_neg_full.txt\", \"../twitter-datasets/train_pos_full.txt\"]\n",
    "\n",
    "hashtags = defaultdict(int)\n",
    "hashtags_per_tweet = []\n",
    "\n",
    "for filename in files:\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file.readlines():\n",
    "            per_tweet = 0\n",
    "            for token in line.split(\" \"):\n",
    "                if token.startswith(\"#\"):\n",
    "                    per_tweet += 1\n",
    "                    hashtags[token] += 1\n",
    "                    \n",
    "            hashtags_per_tweet.append(per_tweet)\n",
    "\n",
    "print(f\"Average hashtags per tweet: {mean(hashtags_per_tweet)}\")\n",
    "\n",
    "print(f\"Median hashtags per tweet: {median(hashtags_per_tweet)}\")\n",
    "\n",
    "hashtags_occurences = list(hashtags.values())\n",
    "for p in [0.5, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.85, 0.9, 0.95, 0.97, 0.98, 0.99, 0.995, 0.999]:\n",
    "    print(f\"{p*100}% of hashtags occur {np.percentile(hashtags_occurences, p*100)} times or less\")\n",
    "\n",
    "print(f\"Total amount of unique hashtags: {len(hashtags)} total amount of hashtags: {sum(hashtags.values())}\")\n",
    "\n",
    "for hashtag, count in sorted(hashtags.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 100:\n",
    "        print(f\"{hashtag}  ->  {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
